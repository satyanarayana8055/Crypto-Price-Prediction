# ğŸ“ˆ Crypto Price Prediction

A complete end-to-end machine learning and MLOps project to predict cryptocurrency prices using a modular, production-grade pipeline. This project combines data ingestion, ETL processing, machine learning, monitoring, and a Flask-based web application. It supports both local development and Docker-based deployment with Airflow, MLflow, and GitHub Actions.

---

## ğŸš€ Features

- ğŸ§ª ML pipeline for training and evaluation
- ğŸ”„ ETL pipeline with Airflow DAGs
- ğŸ§  Model monitoring and drift detection
- ğŸŒ Flask web API for live predictions
- ğŸ“Š MLflow for model tracking
- ğŸ“¦ Docker + Docker Compose for full environment setup
- ğŸ“ CI/CD pipeline with GitHub Actions
- ğŸ› ï¸ Centralized logging and config management
- ğŸ’¾ PostgreSQL database for structured data storage

---

## ğŸ§± Project Structure

```bash
project-root/
â”‚
â”œâ”€â”€ .github/              # GitHub Actions CI/CD setup
â”œâ”€â”€ dags/                 # Airflow DAGs for ETL, training, and monitoring
â”œâ”€â”€ data/                 # Raw, processed, and model storage
â”œâ”€â”€ notebook/             # Jupyter notebooks (EDA, experiments)
â”œâ”€â”€ src/                  # All source code (web, services, ML, monitoring)
â”‚   â”œâ”€â”€ web/              # Flask web app and API
â”‚   â”œâ”€â”€ services/         # Business logic for model and data
â”‚   â”œâ”€â”€ config/           # Configuration classes and environment
â”‚   â”œâ”€â”€ etl/              # ETL pipeline scripts (extract, transform, load)
â”‚   â”œâ”€â”€ ml_components/    # Preprocessing, training, feature engineering
â”‚   â”œâ”€â”€ monitoring/       # Drift detection, performance monitoring
â”‚   â””â”€â”€ utils/            # Logger and helper functions
â”œâ”€â”€ logs/                 # Logs for ETL, app, and model
â”œâ”€â”€ tests/                # Unit tests for different components
â”œâ”€â”€ Dockerfile            # Docker setup for the app
â”œâ”€â”€ docker-compose.yml    # Multi-service Docker orchestration
â”œâ”€â”€ pipeline_config.yaml  # YAML config for pipeline and DAGs
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ setup.py              # Package setup file
â”œâ”€â”€ .env                  # Environment variables
â”œâ”€â”€ .gitignore            # Git ignore rules
â”œâ”€â”€ .dockerignore         # Docker ignore rules
â””â”€â”€ README.md             # You're reading it!

ğŸ§° Setup Instructions

âœ… 1. Clone the Repository
git clone https://github.com/<your-username>/crypto-price-prediction.git
cd crypto-price-prediction

ğŸ 2. Create Conda Environment
conda create -n crypto python=3.10
conda activate crypto

ğŸ“¦ 3. Install Dependencies
pip install -r requirements.txt
pip install -e .
setup.py enables simple module-level imports like:

âš™ï¸ 4. Configuration
.env: Stores environment secrets (API keys, DB credentials)
pipeline_config.yaml: Config for DAGs, file paths, and run settings
config.py: Python classes for global config management

ğŸ³ Docker Usage
ğŸ” Option 1: Pull Image from DockerHub
docker pull <your-dockerhub-username>/crypto-price-prediction:latest
docker-compose up --build

ğŸ§ª Option 2: Build Locally
docker build -t crypto-price-prediction .
docker run -p 5000:5000 crypto-price-prediction

âš ï¸ Docker Volumes
Make sure Docker volumes are mapped for:
Airflow DAGs
Data and logs
pipeline_config.yaml

ğŸ˜ PostgreSQL in Docker
Export CSV from DB:
docker exec -it pg_container bash
su - postgres
psql
\c crypto_db
COPY transactions TO '/tmp/transactions.csv' WITH CSV HEADER;
exit
docker cp pg_container:/tmp/transactions.csv ./transactions.csv

ğŸŒ Run Locally Without Docker
conda activate crypto
python src/web/main.py

ğŸ›« Airflow Quickstart (With Docker)
docker-compose -f docker-compose.airflow.yml up --build
Access: http://localhost:8080
Username/Password: airflow
Ensure proper volume mapping for dags/ and configs.

ğŸ“Š MLflow Usage
mlflow ui --backend-store-uri ./mlruns
Access at: http://localhost:5000
Tracks: models, parameters, metrics, artifacts

ğŸ§  Model Flow
graph TD
main.py --> app.py --> routes.py --> model_service.py & data_service.py

ğŸ› ï¸ Airflow DAGs
etl_pipeline.py	    - Automates data ingestion
model_pipeline.py   - Trains and evaluates models
monitor_pipeline.py - Checks model performance & drift

All DAGs are scheduled and triggered via Airflow and configured using pipeline_config.yaml.

ğŸ”„ GitHub Actions (CI/CD)
Configured workflows in .github/workflows/ci-cd.yml include:
âœ… Code linting with flake8
âœ… Unit tests with pytest
âœ… Docker build and push to DockerHub
âœ… (Optional) Deployment to production server

ğŸ”¬ Testing
Run tests locally using:
pytest tests/
Covers ETL, model, web, and config modules.

ğŸ§  Best Practices Followed
ğŸ”¹ Modular codebase with reusable components
ğŸ”¹ Secrets and configs abstracted via .env and config.py
ğŸ”¹ Centralized logging for debugging and observability
ğŸ”¹ Fully containerized setup using Docker and Docker Compose
ğŸ”¹ DAG-driven automation with Apache Airflow
ğŸ”¹ Model lifecycle tracking via MLflow
ğŸ”¹ Automated CI/CD with GitHub Actions
ğŸ”¹ PostgreSQL as persistent data backend

ğŸ§‘â€ğŸ’» Contributing
Contributions are welcome!
Fork the repository
Create a feature branch
Commit and push your changes
Open a pull request

ğŸ“„ License
This project is licensed under the MIT License.

ğŸ“¬ Contact
Maintainer: satyanarayana
Email: msatya8055@gmail.com
GitHub: satyanarayana8055
